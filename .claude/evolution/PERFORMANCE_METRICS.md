# Agent Performance Metrics & Evaluation

Agent/ìŠ¤í‚¬ ì‚¬ìš© ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì‹œìŠ¤í…œ

**ë²„ì „**: 1.0.0
**ì—…ë°ì´íŠ¸**: 2025-01-14

---

## ğŸ“Š ì„±ëŠ¥ íŒë‹¨ ë©”ì»¤ë‹ˆì¦˜

### 3ê°€ì§€ í‰ê°€ ë°©ì‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ì •ëŸ‰ì  ë©”íŠ¸ë¦­ (ìë™ ì¸¡ì •)                      â”‚
â”‚     - Task completion rate                      â”‚
â”‚     - Execution duration                        â”‚
â”‚     - Error rate                                â”‚
â”‚     - User rating                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ì •ì„±ì  ë©”íŠ¸ë¦­ (LLM-as-Judge)                  â”‚
â”‚     - Output quality (ì¶œë ¥ í’ˆì§ˆ)                 â”‚
â”‚     - Task relevance (ì‘ì—… ê´€ë ¨ì„±)               â”‚
â”‚     - Code quality (ì½”ë“œ í’ˆì§ˆ)                   â”‚
â”‚     - Completeness (ì™„ì „ì„±)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ì¢…í•© ì„±ëŠ¥ ì ìˆ˜                                â”‚
â”‚     - ê°€ì¤‘ í‰ê·  (0-100ì )                        â”‚
â”‚     - ë“±ê¸‰ (S/A/B/C/D/F)                        â”‚
â”‚     - ìƒíƒœ íŒì • (Excellent/Good/Acceptable/Poor) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ ì •ëŸ‰ì  ë©”íŠ¸ë¦­ (ìë™)

### ì¸¡ì • í•­ëª©

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì¸¡ì • ë°©ë²• | ì¢‹ì€ ê¸°ì¤€ |
|--------|------|-----------|----------|
| **Success Rate** | ì„±ê³µ/ì‹¤íŒ¨ìœ¨ | Langfuse traces ë¶„ì„ | â‰¥90% |
| **Avg Duration** | í‰ê·  ì‹¤í–‰ ì‹œê°„ | trace duration ê³„ì‚° | â‰¤2ì´ˆ |
| **Error Rate** | ì—ëŸ¬ìœ¨ | error status ë¹„ìœ¨ | â‰¤5% |
| **User Rating** | ì‚¬ìš©ì í‰ì  | scores.user_rating í‰ê·  | â‰¥4/5 |
| **Effectiveness** | íš¨ê³¼ì„± | scores.effectiveness í‰ê·  | â‰¥0.8 |
| **P95 Duration** | 95 percentile ì‹œê°„ | ìƒìœ„ 5% ì œì™¸ | â‰¤5ì´ˆ |

### ìë™ ìˆ˜ì§‘

```python
from track_agent_usage import get_tracker

tracker = get_tracker()

# Agent ì‹¤í–‰ (ìë™ ê¸°ë¡)
with tracker.track("context7-engineer", phase="Phase 0", task="Verify docs"):
    result = agent.run()
    # â†’ duration, status ìë™ ì¸¡ì •

# í”¼ë“œë°± ìˆ˜ì§‘
tracker.collect_feedback(
    agent="context7-engineer",
    rating=5,  # 1-5
    effectiveness=0.95  # 0-1
)
```

---

## 2ï¸âƒ£ ì •ì„±ì  ë©”íŠ¸ë¦­ (LLM-as-Judge)

### Claude API ê¸°ë°˜ ìë™ í‰ê°€

**ëª©ì **: ì¶œë ¥ë¬¼ì˜ í’ˆì§ˆì„ AIê°€ ìë™ìœ¼ë¡œ í‰ê°€

### í‰ê°€ í•­ëª©

| í•­ëª© | ì„¤ëª… | í‰ê°€ ê¸°ì¤€ |
|------|------|-----------|
| **Quality** | ì¶œë ¥ í’ˆì§ˆ | ê°€ë…ì„±, ì „ë¬¸ì„±, êµ¬ì¡° |
| **Relevance** | ì‘ì—… ê´€ë ¨ì„± | Task ìš”êµ¬ì‚¬í•­ ì¶©ì¡±ë„ |
| **Completeness** | ì™„ì „ì„± | í•„ìš”í•œ ì •ë³´ í¬í•¨ ì—¬ë¶€ |
| **Accuracy** | ì •í™•ì„± | ì‚¬ì‹¤ ì˜¤ë¥˜, ë…¼ë¦¬ ì˜¤ë¥˜ ì—†ìŒ |

### ì‚¬ìš© ë°©ë²•

```python
from llm_judge import LLMJudge

judge = LLMJudge()

# Agent ì¶œë ¥ í‰ê°€
score = judge.evaluate_output(
    agent="context7-engineer",
    task="Verify React 18 documentation",
    output="React 18 introduces Suspense, Concurrent rendering...",
    expected="Comprehensive documentation verification"
)

print(f"Quality: {score.quality}/10")
print(f"Overall: {score.overall_score:.1f}/10")
print(f"Reasoning: {score.reasoning}")
```

### ì½”ë“œ í’ˆì§ˆ í‰ê°€

```python
# ìƒì„±ëœ ì½”ë“œ í‰ê°€
code = """
def calculate_total(items):
    return sum(item.price * item.quantity for item in items)
"""

score = judge.evaluate_code_quality(code, language="python")
# â†’ Quality, Relevance, Completeness, Accuracy
```

### A/B í…ŒìŠ¤íŠ¸

```python
# ë‘ ë²„ì „ ë¹„êµ
comparison = judge.compare_outputs(
    output_a=agent_v1_output,
    output_b=agent_v2_output,
    task="Generate unit tests"
)

print(f"Winner: {comparison['winner']}")  # "A" or "B" or "tie"
print(f"Reasoning: {comparison['reasoning']}")
```

---

## 3ï¸âƒ£ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜

### ê³„ì‚° ê³µì‹

```python
performance_score = (
    success_rate * 0.30 +      # 30%
    user_rating * 0.25 +       # 25%
    effectiveness * 0.20 +     # 20%
    speed_score * 0.15 +       # 15% (inverse duration)
    (1 - error_rate) * 0.10    # 10% (inverse error)
) * 100
```

### ë“±ê¸‰ ì²´ê³„

| ì ìˆ˜ | ë“±ê¸‰ | ìƒíƒœ | ì˜ë¯¸ |
|------|------|------|------|
| 90-100 | S | âœ… Excellent | ìµœìƒê¸‰, ê°œì„  ë¶ˆí•„ìš” |
| 80-89 | A | âœ”ï¸ Good | ìš°ìˆ˜, ì†Œí­ ê°œì„  |
| 70-79 | B | âš ï¸ Acceptable | ì–‘í˜¸, ê°œì„  ê¶Œì¥ |
| 60-69 | C | âš ï¸ Acceptable | ë³´í†µ, ê°œì„  í•„ìš” |
| 50-59 | D | âŒ Needs Improvement | ë¯¸í¡, ì¦‰ì‹œ ê°œì„  |
| 0-49 | F | âŒ Needs Improvement | ë¶ˆëŸ‰, ê¸´ê¸‰ ê°œì„  |

### ì˜ˆì‹œ

```python
from evaluate_agent_performance import AgentEvaluator

evaluator = AgentEvaluator()

# Agent í‰ê°€
metrics = evaluator.get_agent_metrics("context7-engineer", days=7)

print(f"Score: {metrics.performance_score}/100")  # 85.2
print(f"Grade: {metrics.grade}")  # "A"
print(f"Status: {metrics.status}")  # "âœ”ï¸ Good"
```

---

## ğŸ“ˆ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹ ê·œ Agent í‰ê°€

```bash
# 1ì£¼ì¼ ì‚¬ìš© í›„ í‰ê°€
python .claude/evolution/scripts/evaluate_agent_performance.py \
    --agent new-agent \
    --days 7 \
    --report

# ì¶œë ¥:
# Score: 72.5/100 (Grade: B)
# Status: âš ï¸ Acceptable
#
# Recommendations:
# 1. Improve success rate (75% â†’ 90%+)
# 2. Reduce avg duration (3.2s â†’ 2s)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Agent ë¹„êµ

```bash
# Phase 0ì—ì„œ 3ê°œ agent ë¹„êµ
python .claude/evolution/scripts/evaluate_agent_performance.py \
    --compare \
    --phase "Phase 0"

# ì¶œë ¥:
# Agent                      Score    Grade  Status
# ========================================================
# context7-engineer          88.3     A      âœ”ï¸ Good
# seq-engineer               71.2     B      âš ï¸ Acceptable
# backend-architect          84.5     A      âœ”ï¸ Good
#
# ğŸ¥‡ Best: context7-engineer
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: Baseline ì„¤ì •

```bash
# í˜„ì¬ ì„±ëŠ¥ì„ baselineìœ¼ë¡œ ì„¤ì •
python .claude/evolution/scripts/evaluate_agent_performance.py \
    --agent context7-engineer \
    --baseline

# â†’ config/context7-engineer-baseline.json ì €ì¥
# í–¥í›„ ê°œì„  ì „/í›„ ë¹„êµ ê°€ëŠ¥
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: LLM-as-Judge í‰ê°€

```python
# Agent ì¶œë ¥ ìë™ í‰ê°€
judge = LLMJudge()

with tracker.track("playwright-engineer", phase="Phase 2"):
    test_code = generate_e2e_tests()

# LLM Judgeë¡œ ì½”ë“œ í’ˆì§ˆ í‰ê°€
score = judge.evaluate_code_quality(test_code, language="javascript")

# Langfuseì— score ê¸°ë¡
tracker.current_trace.score(
    name="llm_judge_quality",
    value=score.normalized_score,  # 0-1
    comment=score.reasoning
)
```

---

## ğŸ¯ ê°œì„  ê¶Œì¥ ê¸°ì¤€

### ìë™ ê¶Œì¥ ì•¡ì…˜

| ì¡°ê±´ | ê¶Œì¥ ì•¡ì…˜ | ìš°ì„ ìˆœìœ„ |
|------|----------|---------|
| Success rate < 80% | ì‹¤íŒ¨ trace ë¶„ì„, ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„  | ğŸ”´ High |
| Avg duration > 3s | ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬, ìµœì í™” | ğŸŸ¡ Medium |
| User rating < 3.5/5 | í”¼ë“œë°± ë¶„ì„, instruction ê°œì„  | ğŸ”´ High |
| Error rate > 10% | ì—ëŸ¬ ë¡œê·¸ ë¶„ì„, retry ë¡œì§ ì¶”ê°€ | ğŸ”´ High |
| LLM Judge score < 6/10 | ì¶œë ¥ í’ˆì§ˆ ê°œì„ , í”„ë¡¬í”„íŠ¸ íŠœë‹ | ğŸŸ¡ Medium |

### Phase 2 ìë™ ê°œì„  (í–¥í›„)

```python
# PromptAgentë¡œ ìë™ ìµœì í™”
python .claude/evolution/scripts/optimize_agents.py --weekly

# ì‘ë™:
# 1. ì„±ëŠ¥ í‰ê°€ (ì •ëŸ‰ + LLM Judge)
# 2. ë‚®ì€ ì ìˆ˜ agent ì‹ë³„
# 3. PromptAgentë¡œ instruction ê°œì„ 
# 4. A/B í…ŒìŠ¤íŠ¸ (v1.0 vs v1.1)
# 5. ìŠ¹ì ìë™ PR ìƒì„±
```

---

## ğŸ“Š ëŒ€ì‹œë³´ë“œ í™œìš©

### Langfuseì—ì„œ í™•ì¸

**Traces**:
- Filter: `output.status = "success"`
- Group by: `metadata.agent`
- Metric: Avg duration

**Scores**:
- Filter: `scores.user_rating < 0.8`
- Sort by: Agent
- Identify: ê°œì„  í•„ìš” agent

**Analytics**:
- Chart: Agentë³„ ì„±ê³µë¥  ì¶”ì´
- Chart: Phaseë³„ í‰ê·  duration
- Chart: ì‹œê°„ì— ë”°ë¥¸ user rating ë³€í™”

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ & ê²€ì¦

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€ì¦

```bash
# 1. ë°ëª¨ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°)
python .claude/evolution/scripts/example_integration.py

# 2. ì„±ëŠ¥ í‰ê°€
python .claude/evolution/scripts/evaluate_agent_performance.py \
    --agent context7-engineer

# 3. LLM Judge í…ŒìŠ¤íŠ¸
python .claude/evolution/scripts/llm_judge.py
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸ & ì—°êµ¬

- **LLM-as-Judge**: [Judging LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)
- **PromptAgent**: [Strategic Planning with LLMs](https://arxiv.org/abs/2310.16427)
- **AgentBench**: [Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688)

### ê´€ë ¨ ë„êµ¬

- **Langfuse**: https://langfuse.com/docs/scores
- **DeepEval**: https://github.com/confident-ai/deepeval
- **Opik**: https://github.com/comet-ml/opik

---

## ğŸ”„ ê°œì„  ë¡œë“œë§µ

### Phase 1 (í˜„ì¬)
- âœ… ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- âœ… LLM-as-Judge êµ¬í˜„
- âœ… ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
- âœ… ë“±ê¸‰ ì²´ê³„

### Phase 2 (2-3ì£¼)
- ğŸ”œ PromptAgent í†µí•©
- ğŸ”œ ìë™ A/B í…ŒìŠ¤íŠ¸
- ğŸ”œ Baseline ë¹„êµ
- ğŸ”œ ìë™ ê°œì„  PR

### Phase 3 (1-2ê°œì›”)
- ğŸ”œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ğŸ”œ ì•Œë¦¼ ì‹œìŠ¤í…œ (ì„±ëŠ¥ ì €í•˜ ê°ì§€)
- ğŸ”œ íŠ¸ë Œë“œ ë¶„ì„
- ğŸ”œ ì˜ˆì¸¡ ëª¨ë¸ (ì„±ëŠ¥ ì˜ˆì¸¡)

---

## ğŸ’¡ Best Practices

### 1. Baseline ì„¤ì •
```bash
# ìƒˆ agent ì¶”ê°€ ì‹œ baseline ì„¤ì •
python evaluate_agent_performance.py --agent new-agent --baseline
```

### 2. ì£¼ê°„ ë¦¬ë·°
```bash
# ë§¤ì£¼ ì „ì²´ agent ì„±ëŠ¥ í™•ì¸
python evaluate_agent_performance.py --compare
```

### 3. ê°œì„  ì „/í›„ ë¹„êµ
```python
# Before
metrics_before = evaluator.get_agent_metrics("agent", days=7)

# ê°œì„  ì‘ì—… (instruction ìˆ˜ì • ë“±)

# After
metrics_after = evaluator.get_agent_metrics("agent", days=1)

improvement = metrics_after.performance_score - metrics_before.performance_score
print(f"Improvement: +{improvement:.1f} points")
```

### 4. LLM Judge í™œìš©
```python
# ëª¨ë“  agent ì¶œë ¥ì— ìë™ ì ìš©
with tracker.track("agent", ...):
    output = agent.run()

    # ìë™ í’ˆì§ˆ í‰ê°€
    score = judge.evaluate_output(agent, task, output)

    tracker.current_trace.score(
        name="llm_judge",
        value=score.normalized_score
    )
```

---

**ì‘ì„±ì**: Claude Code
**ì—…ë°ì´íŠ¸**: 2025-01-14
**ë²„ì „**: 1.0.0
