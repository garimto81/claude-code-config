#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM-as-Judge: Agent ì¶œë ¥ í’ˆì§ˆ ìë™ í‰ê°€
Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ Agent ì¶œë ¥ë¬¼ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€

Usage:
    from llm_judge import LLMJudge

    judge = LLMJudge()

    score = judge.evaluate_output(
        agent="context7-engineer",
        task="Verify React 18 docs",
        output="React 18 introduces Suspense, Concurrent rendering...",
        expected="Documentation verification"
    )

    print(f"Quality Score: {score.quality}/10")
    print(f"Relevance: {score.relevance}/10")
    print(f"Completeness: {score.completeness}/10")
"""

import os
from dataclasses import dataclass
from typing import Optional
from pathlib import Path

from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / '.env')

try:
    import anthropic
except ImportError:
    print("âŒ anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install anthropic")
    exit(1)


@dataclass
class JudgeScore:
    """LLM Judge í‰ê°€ ì ìˆ˜"""
    quality: float  # 0-10
    relevance: float  # 0-10
    completeness: float  # 0-10
    accuracy: float  # 0-10
    reasoning: str  # í‰ê°€ ê·¼ê±°

    @property
    def overall_score(self) -> float:
        """ì¢…í•© ì ìˆ˜ (0-10)"""
        return (self.quality + self.relevance + self.completeness + self.accuracy) / 4

    @property
    def normalized_score(self) -> float:
        """ì •ê·œí™” ì ìˆ˜ (0-1)"""
        return self.overall_score / 10


class LLMJudge:
    """LLM-as-Judge í‰ê°€ê¸°"""

    def __init__(self):
        """Initialize Claude API client"""
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment")

        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-20250514"

    def evaluate_output(
        self,
        agent: str,
        task: str,
        output: str,
        expected: Optional[str] = None,
        context: Optional[str] = None
    ) -> JudgeScore:
        """
        Agent ì¶œë ¥ í’ˆì§ˆ í‰ê°€

        Args:
            agent: Agent ì´ë¦„
            task: ìˆ˜í–‰í•œ ì‘ì—…
            output: Agent ì¶œë ¥ ê²°ê³¼
            expected: ê¸°ëŒ€ ê²°ê³¼ (optional)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (optional)

        Returns:
            JudgeScore
        """
        prompt = self._build_evaluation_prompt(agent, task, output, expected, context)

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )

            result = response.content[0].text
            score = self._parse_evaluation(result)
            return score

        except Exception as e:
            print(f"âš ï¸ LLM Judge error: {e}")
            # Fallback to neutral score
            return JudgeScore(
                quality=5.0,
                relevance=5.0,
                completeness=5.0,
                accuracy=5.0,
                reasoning=f"Error during evaluation: {e}"
            )

    def _build_evaluation_prompt(
        self,
        agent: str,
        task: str,
        output: str,
        expected: Optional[str],
        context: Optional[str]
    ) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""You are an expert evaluator assessing the quality of AI agent outputs.

**Agent**: {agent}
**Task**: {task}
"""

        if expected:
            prompt += f"**Expected Result**: {expected}\n"

        if context:
            prompt += f"**Context**: {context}\n"

        prompt += f"""
**Agent Output**:
```
{output}
```

Evaluate the agent's output on the following criteria (0-10 scale):

1. **Quality**: How well-written and professional is the output?
2. **Relevance**: How relevant is the output to the task?
3. **Completeness**: Does the output fully address the task?
4. **Accuracy**: Is the information accurate and correct?

Provide your evaluation in the following JSON format:

```json
{{
  "quality": <score 0-10>,
  "relevance": <score 0-10>,
  "completeness": <score 0-10>,
  "accuracy": <score 0-10>,
  "reasoning": "<brief explanation of scores>"
}}
```

Be objective and fair in your assessment.
"""
        return prompt

    def _parse_evaluation(self, result: str) -> JudgeScore:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        import json
        import re

        # JSON ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', result, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # JSON ë¸”ë¡ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì‹œë„
            json_str = result

        try:
            data = json.loads(json_str)
            return JudgeScore(
                quality=float(data.get('quality', 5.0)),
                relevance=float(data.get('relevance', 5.0)),
                completeness=float(data.get('completeness', 5.0)),
                accuracy=float(data.get('accuracy', 5.0)),
                reasoning=data.get('reasoning', 'No reasoning provided')
            )
        except json.JSONDecodeError:
            # Fallback
            return JudgeScore(
                quality=5.0,
                relevance=5.0,
                completeness=5.0,
                accuracy=5.0,
                reasoning="Failed to parse evaluation"
            )

    def evaluate_code_quality(self, code: str, language: str = "python") -> JudgeScore:
        """
        ìƒì„±ëœ ì½”ë“œ í’ˆì§ˆ í‰ê°€

        Args:
            code: ì½”ë“œ
            language: í”„ë¡œê·¸ë˜ë° ì–¸ì–´

        Returns:
            JudgeScore
        """
        prompt = f"""Evaluate the quality of this {language} code:

```{language}
{code}
```

Rate on these criteria (0-10):

1. **Quality**: Code readability, style, best practices
2. **Relevance**: Does it solve the intended problem?
3. **Completeness**: Is it complete and functional?
4. **Accuracy**: Is the logic correct?

Provide JSON format:
```json
{{
  "quality": <score>,
  "relevance": <score>,
  "completeness": <score>,
  "accuracy": <score>,
  "reasoning": "<explanation>"
}}
```
"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )

            result = response.content[0].text
            return self._parse_evaluation(result)

        except Exception as e:
            return JudgeScore(5.0, 5.0, 5.0, 5.0, f"Error: {e}")

    def compare_outputs(self, output_a: str, output_b: str, task: str) -> Dict:
        """
        ë‘ Agent ì¶œë ¥ ë¹„êµ

        Args:
            output_a: Agent A ì¶œë ¥
            output_b: Agent B ì¶œë ¥
            task: ì‘ì—… ì„¤ëª…

        Returns:
            ë¹„êµ ê²°ê³¼
        """
        prompt = f"""Compare two agent outputs for this task: {task}

**Output A**:
```
{output_a}
```

**Output B**:
```
{output_b}
```

Which output is better? Provide:

```json
{{
  "winner": "A" or "B" or "tie",
  "score_a": <0-10>,
  "score_b": <0-10>,
  "reasoning": "<explanation>"
}}
```
"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )

            result = response.content[0].text

            import json
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', result, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(1))
                return data
            else:
                return {
                    "winner": "tie",
                    "score_a": 5.0,
                    "score_b": 5.0,
                    "reasoning": "Failed to parse"
                }

        except Exception as e:
            return {
                "winner": "tie",
                "score_a": 5.0,
                "score_b": 5.0,
                "reasoning": f"Error: {e}"
            }


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª Testing LLM Judge...\n")

    judge = LLMJudge()

    # Test 1: ë¬¸ì„œ ê²€ì¦ í‰ê°€
    print("Test 1: Document Verification")
    score = judge.evaluate_output(
        agent="context7-engineer",
        task="Verify React 18 documentation",
        output="""React 18 introduces several new features:
        1. Concurrent Rendering
        2. Automatic Batching
        3. Suspense for data fetching
        4. New hooks: useId, useDeferredValue, useTransition

        All features are production-ready and documented at react.dev""",
        expected="Comprehensive verification of React 18 features"
    )

    print(f"Quality: {score.quality}/10")
    print(f"Relevance: {score.relevance}/10")
    print(f"Completeness: {score.completeness}/10")
    print(f"Accuracy: {score.accuracy}/10")
    print(f"Overall: {score.overall_score:.1f}/10")
    print(f"Reasoning: {score.reasoning}\n")

    # Test 2: ì½”ë“œ í’ˆì§ˆ í‰ê°€
    print("Test 2: Code Quality")
    code = """
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price * item.quantity
    return total
"""
    score = judge.evaluate_code_quality(code, language="python")

    print(f"Code Quality: {score.overall_score:.1f}/10")
    print(f"Reasoning: {score.reasoning}\n")

    # Test 3: ì¶œë ¥ ë¹„êµ
    print("Test 3: Output Comparison")
    output_a = "React 18 has Suspense"
    output_b = "React 18 introduces Suspense for data fetching, Concurrent rendering, and automatic batching"

    comparison = judge.compare_outputs(output_a, output_b, "Describe React 18 features")
    print(f"Winner: {comparison['winner']}")
    print(f"Score A: {comparison['score_a']}/10")
    print(f"Score B: {comparison['score_b']}/10")
    print(f"Reasoning: {comparison['reasoning']}")

    print("\nâœ… All tests completed!")


if __name__ == "__main__":
    from typing import Dict
    main()
